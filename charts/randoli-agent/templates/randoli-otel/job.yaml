{{- if eq (include "apply-open-telemetry-crs" . | trim) "true" -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: apply-otel-crs
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 0 
  template:
    spec:
      serviceAccountName: otel-cr-applier
      containers:
      - name: apply-crs
        image: bitnami/kubectl:latest
        command:
        - /bin/bash
        - -c
        - |
          echo "⏳ Waiting for Operator pod..."
          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=opentelemetry-operator,app.kubernetes.io/component=controller-manager -n {{ .Release.Namespace }} --timeout=120s
          echo "✅ Operator is ready. Applying CRs..."
          kubectl apply -f /crs/otel-collector.yaml -n {{ .Release.Namespace }}
          kubectl apply -f /crs/instrumentation.yaml -n {{ .Release.Namespace }}
        volumeMounts:
        - name: crs
          mountPath: /crs
      restartPolicy: Never
      volumes:
      - name: crs
        configMap:
          name: otel-crs
      {{- with .Values.tolerations }}
      tolerations: {{ toYaml . | nindent 8 }}
      {{- end }}        
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-crs
data:
  otel-collector.yaml: |
    apiVersion: opentelemetry.io/v1beta1
    kind: OpenTelemetryCollector
    metadata:
      name: randoli-otel
    spec:
      {{- with .Values.tolerations }}
      tolerations: {{ toYaml . | nindent 8 }}
      {{- end }}
      image: docker.io/otel/opentelemetry-collector-k8s:0.122.1  
      upgradeStrategy: automatic
      mode: statefulset
      hostNetwork: true
      serviceAccount: randoli-otel-collector
      autoscaler:
        maxReplicas: 5
        minReplicas: 2
        targetMemoryUtilization: 85
        targetCPUUtilization: 90
      resources:
        requests:
          memory: "500Mi"
        limits:
          memory: "1000Mi"
      config:
        receivers:
          otlp:
            protocols:
              grpc:
                endpoint: 0.0.0.0:4317
              http:
                endpoint: 0.0.0.0:4318
                cors:
                  allowed_origins:
                  - "*"
                  allowed_headers: ["*"]
          prometheus:
            config:
              scrape_configs:
              - job_name: 'otel-collector'
                scrape_interval: 10s
                static_configs:
                - targets: [ '0.0.0.0:8888' ]
                metric_relabel_configs:
                - action: labeldrop
                  regex: (id|name)
                - action: labelmap
                  regex: label_(.+)
                  replacement: $$1

        exporters:
          ## Create an exporter to Jaeger using the standard `otlp` export format
          otlp/jaeger:
            endpoint: 'randoli-rok-jaeger-collector.{{ .Release.Namespace }}.svc:4317'
            tls:
              insecure: true

          ## Prometheus
          otlphttp:
            endpoint: "{{ include "prometheus-server-endpoint" . }}/api/v1/otlp"
            
          debug:
            verbosity: basic
            sampling_initial: 5
            sampling_thereafter: 200

        processors:
          # Add new attributes to identify Kubernetes controller resource type and the name
          transform/controller_resource:
            error_mode: ignore
            trace_statements:
              - context: resource
                error_mode: propagate
                conditions:
                  - resource.attributes["k8s.deployment.name"] != nil
                statements:
                  - set(resource.attributes["k8s.controller.name"], resource.attributes["k8s.deployment.name"])
                  - set(resource.attributes["k8s.controller.type"], "deployment")
      
              - context: resource
                error_mode: propagate
                conditions:
                  - resource.attributes["k8s.daemonset.name"] != nil
                statements:
                  - set(resource.attributes["k8s.controller.name"], resource.attributes["k8s.daemonset.name"])
                  - set(resource.attributes["k8s.controller.type"], "daemonset")
      
              - context: resource
                error_mode: propagate
                conditions:
                  - resource.attributes["k8s.statefulset.name"] != nil
                statements:
                  - set(resource.attributes["k8s.controller.name"], resource.attributes["k8s.statefulset.name"])
                  - set(resource.attributes["k8s.controller.type"], "statefulset")

          #Construct the container name from available metadata. Only works if the container.name attribute is available in the incoming span
          transform/container_image:
            error_mode: ignore
            trace_statements:
              - context: resource
                error_mode: propagate
                conditions:
                  - resource.attributes["container.image.name"] != nil
                  - resource.attributes["container.image.tag"] != nil
                statements:
                  - set(resource.attributes["container.image"], Concat([resource.attributes["container.image.name"], resource.attributes["container.image.tag"]], ":") )
    
          resource:
            attributes:
            - key: service.instance.id
              from_attribute: k8s.pod.uid
              action: insert
          memory_limiter:
            check_interval: 5s
            limit_mib: 500
            spike_limit_mib: 256
          batch:
            send_batch_max_size: 10000
            timeout: 5s
          k8sattributes:
            auth_type: "serviceAccount"
            passthrough: false
            extract:
              metadata:
                - k8s.pod.name
                - k8s.pod.uid
                - k8s.deployment.name
                - k8s.namespace.name
                - k8s.node.name
                - k8s.pod.start_time
                - container.image.name
                - container.image.tag
                - k8s.container.name

          # attributes processors to define the span type
          attributes/http:
            actions:
              - key: span.type
                value: http
                action: insert
            include:
              match_type: regexp
              attributes:
                - key: http.method
                  value: .+ # Match any non-empty value for http.method
          attributes/db:
            actions:
              - key: span.type
                value: database
                action: insert
            include:
              match_type: regexp
              attributes:
                - key: db.system
                  value: .+ # Match any non-empty value for http.method
        
          attributes/kafka:
            actions:
              - key: span.type
                value: messaging
                action: insert
            include:
              match_type: regexp
              attributes:
                - key: messaging.system
                  value: .+ # Match any non-empty value for http.method

          metricstransform/netobserv:
            transforms:
              # rename all netobserv* metrics to randoli metrics format
              # instead of regular $ use double dollar $$. Because $ is treated as a special character.
              # wrap the group name/number with braces
              - include: ^netobserv_(.*)$$
                match_type: regexp
                action: update
                new_name: randoli.metrics.otel.netobserv.$${1}

        connectors:
          spanmetrics:
            dimensions:
            - name: k8s.controller.name
              default: unknown
            - name: k8s.controller.type
              default: unknown
            - name: k8s.namespace.name
              default: unknown
            - name: span.type
              default: unknown
            - name: server.address
            - name: k8s.pod.name
              default: unknown
            - name: k8s.container.name
              default: unknown
            - name: container.image
              default: unknown

        service:
          pipelines:
            traces:
              receivers: [otlp]
              processors: 
                - memory_limiter
                - k8sattributes
                - resource
                - transform/controller_resource
                - transform/container_image
                - attributes/http
                - attributes/db
                - attributes/kafka
                - batch
              exporters: [debug, spanmetrics, otlp/jaeger]
            metrics:
              receivers: [otlp, prometheus, spanmetrics]
              processors:
                - memory_limiter
                - resource
                - metricstransform/netobserv
                - batch
              exporters: [debug, otlphttp]
            logs:
              receivers: [otlp]
              processors: [memory_limiter, resource, batch]
              exporters: [debug]
      targetAllocator:
        {{- with .Values.tolerations }}
        tolerations: {{ toYaml . | nindent 10 }}
        {{- end }} 
        enabled: true
        prometheusCR:
          enabled: true
          serviceMonitorSelector: {}
          podMonitorSelector: {}
        serviceAccount: opentelemetry-targetallocator
  instrumentation.yaml: |
    apiVersion: opentelemetry.io/v1alpha1
    kind: Instrumentation
    metadata:
      name: otel-instrumentation
    spec:
      exporter:
        endpoint: http://randoli-otel-collector.{{ .Release.Namespace }}.svc:4317
      propagators:
        - tracecontext
        - baggage
        - b3
      sampler:
        type: parentbased_traceidratio
        argument: "0.25"
      python:
        env:
          # Required if endpoint is set to 4317.
          # Python autoinstrumentation uses http/proto by default
          # so data must be sent to 4318 instead of 4317.
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://randoli-otel-collector.{{ .Release.Namespace }}.svc:4318
      dotnet:
        env:
          # Required if endpoint is set to 4317.
          # Dotnet autoinstrumentation uses http/proto by default
          # See https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/blob/888e2cd216c77d12e56b54ee91dafbc4e7452a52/docs/config.md#otlp
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://randoli-otel-collector.{{ .Release.Namespace }}.svc:4318
      go:
        env:
          # Required if endpoint is set to 4317.
          # Go autoinstrumentation uses http/proto by default
          # so data must be sent to 4318 instead of 4317.
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://randoli-otel-collector.{{ .Release.Namespace }}.svc:4318
{{end}}             
---